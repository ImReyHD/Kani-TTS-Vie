{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ğŸ™ï¸ Chuáº©n bá»‹ dá»¯ liá»‡u VieNeu-TTS-140h cho Kani TTS\n",
        "\n",
        "Notebook nÃ y sáº½:\n",
        "1. âœ… Táº£i dataset VieNeu-TTS-140h tá»« HuggingFace\n",
        "2. âœ… Encode audio thÃ nh NanoCodec tokens (4 layers)\n",
        "3. âœ… LÆ°u dataset Ä‘Ã£ xá»­ lÃ½\n",
        "4. âœ… Upload lÃªn HuggingFace Hub\n",
        "\n",
        "**YÃªu cáº§u:** GPU (T4/V100/A100) trÃªn Google Colab hoáº·c Kaggle\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "âœ… ÄÃ£ cÃ i Ä‘áº·t táº¥t cáº£ dependencies!\n"
          ]
        }
      ],
      "source": [
        "# CÃ i Ä‘áº·t dependencies\n",
        "%uv pip install -q omegaconf numpy datasets==3.6.0 torch librosa soundfile tqdm\n",
        "%uv pip install -q nemo_toolkit[all]==2.4.0\n",
        "%uv pip install -q --upgrade protobuf==5.29.2\n",
        "\n",
        "print(\"âœ… ÄÃ£ cÃ i Ä‘áº·t táº¥t cáº£ dependencies!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… GPU available: NVIDIA L4\n",
            "   GPU Memory: 22.03 GB\n"
          ]
        }
      ],
      "source": [
        "# Kiá»ƒm tra GPU\n",
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"âœ… GPU available: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"   GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
        "else:\n",
        "    print(\"âš ï¸  No GPU found! Processing will be very slow.\")\n",
        "    print(\"   Please enable GPU: Runtime > Change runtime type > GPU\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## BÆ°á»›c 1: Login HuggingFace\n",
        "\n",
        "Cáº§n login Ä‘á»ƒ táº£i dataset vÃ  upload káº¿t quáº£ lÃªn HuggingFace Hub.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Login HuggingFace\n",
        "from huggingface_hub import login\n",
        "\n",
        "import os\n",
        "\n",
        "if \"HF_TOKEN\" in os.environ:\n",
        "    login(token=os.environ[\"HF_TOKEN\"])\n",
        "    print(\"âœ… ÄÃ£ login HuggingFace tá»« environment variable\")\n",
        "else:\n",
        "    print(\"âš ï¸  ChÆ°a cÃ³ HF_TOKEN trong environment.\")\n",
        "    print(\"   Vui lÃ²ng thÃªm token vÃ o Secrets hoáº·c uncomment dÃ²ng trÃªn Ä‘á»ƒ nháº­p trá»±c tiáº¿p.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## BÆ°á»›c 2: Import thÆ° viá»‡n vÃ  setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Using device: cuda\n",
            "âœ… Output directory: shards\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "import gzip\n",
        "import numpy as np\n",
        "import torch\n",
        "from tqdm.auto import tqdm\n",
        "from datasets import load_dataset, Audio, disable_progress_bars\n",
        "from nemo.collections.tts.models import AudioCodecModel\n",
        "\n",
        "# Disable verbose logs\n",
        "disable_progress_bars()\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Setup\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"âœ… Using device: {device}\")\n",
        "\n",
        "# Táº¡o thÆ° má»¥c output\n",
        "OUT_DIR = \"shards\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "print(f\"âœ… Output directory: {OUT_DIR}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## BÆ°á»›c 3: Load NanoCodec Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ“¥ Loading NanoCodec model: nvidia/nemo-nano-codec-22khz-0.6kbps-12.5fps\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bcc1eb0a89704cd3a9372a623e5ad662",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "nemo-nano-codec-22khz-0.6kbps-12.5fps.ne(â€¦):   0%|          | 0.00/422M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[NeMo W 2025-11-06 09:52:22 nemo_logging:405] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
            "    Train config : \n",
            "    dataset:\n",
            "      dataset_type: tarred_vocoder\n",
            "      dataset_args:\n",
            "        dataset_meta:\n",
            "          mls_english:\n",
            "            manifest_path: /lustre/fsw/portfolios/llmservice/projects/llmservice_nemo_speechlm/data/TTS/mls_english/filtered_22khz/tarred_audio/train_manifest.json\n",
            "            tar_filepath: /lustre/fsw/portfolios/llmservice/projects/llmservice_nemo_speechlm/data/TTS/mls_english/filtered_22khz/tarred_audio/audio_{0..1999}.tar\n",
            "          cv:\n",
            "            manifest_path: /lustre/fsw/portfolios/llmservice/projects/llmservice_nemo_speechlm/data/TTS/commonvoice13/22khz/tarred_audio/train_manifest.json\n",
            "            tar_filepath: /lustre/fsw/portfolios/llmservice/projects/llmservice_nemo_speechlm/data/TTS/commonvoice13/22khz/tarred_audio/audio_{0..279}.tar\n",
            "        sample_rate: 22050\n",
            "        n_samples: 24696\n",
            "        min_duration: 0.4\n",
            "        max_duration: null\n",
            "        shard_strategy: replicate\n",
            "        sample_type: weighted_random\n",
            "        sample_args:\n",
            "          batch_size: 32\n",
            "          steps_per_epoch: 1000\n",
            "          dataset_weights:\n",
            "          - 0.75\n",
            "          - 0.25\n",
            "        shuffle_n: 10000\n",
            "    dataloader_params:\n",
            "      batch_size: 32\n",
            "      drop_last: true\n",
            "      num_workers: 4\n",
            "    \n",
            "[NeMo W 2025-11-06 09:52:22 nemo_logging:405] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
            "    Validation config : \n",
            "    dataset:\n",
            "      dataset_type: vocoder\n",
            "      dataset_args:\n",
            "        sample_rate: 22050\n",
            "        n_samples: null\n",
            "        min_duration: null\n",
            "        max_duration: null\n",
            "        trunc_duration: 10.0\n",
            "        dataset_meta:\n",
            "          mls_english:\n",
            "            manifest_path: /lustre/fsw/portfolios/llmservice/projects/llmservice_nemo_speechlm/data/TTS/mls_english/filtered_22khz/val_manifest_unseen.json\n",
            "            audio_dir: /lustre/fsw/portfolios/llmservice/projects/llmservice_nemo_speechlm/data/TTS/mls_english/filtered_22khz/audio_22khz\n",
            "          cv:\n",
            "            manifest_path: /lustre/fsw/portfolios/llmservice/projects/llmservice_nemo_speechlm/data/TTS/commonvoice13/22khz/val_manifest.json\n",
            "            audio_dir: /lustre/fsw/portfolios/llmservice/projects/llmservice_nemo_speechlm/data/TTS/commonvoice13/22khz/audio_22khz\n",
            "    dataloader_params:\n",
            "      batch_size: 4\n",
            "      num_workers: 2\n",
            "    \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[NeMo I 2025-11-06 09:52:22 nemo_logging:393] Vector quantizer does not support commit loss.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a5ba11d52cca499486a1d828f71dc9b8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8d60d60d5f5a4f448459d6334fa3d98b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/378M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[NeMo I 2025-11-06 09:52:28 nemo_logging:393] PADDING: 1\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7f26383a8a1e42ec95ffe07104c97e1d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/378M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[NeMo I 2025-11-06 09:52:58 nemo_logging:393] PADDING: 1\n",
            "[NeMo I 2025-11-06 09:52:58 nemo_logging:393] PADDING: 1\n",
            "[NeMo I 2025-11-06 09:52:58 nemo_logging:393] PADDING: 1\n",
            "[NeMo I 2025-11-06 09:52:58 nemo_logging:393] PADDING: 1\n",
            "[NeMo I 2025-11-06 09:52:58 nemo_logging:393] PADDING: 1\n",
            "[NeMo I 2025-11-06 09:52:59 nemo_logging:393] Model AudioCodecModel was successfully restored from /root/.cache/huggingface/hub/models--nvidia--nemo-nano-codec-22khz-0.6kbps-12.5fps/snapshots/5c8e22ed763c14d81337fbe6ca74062f3d10f7e5/nemo-nano-codec-22khz-0.6kbps-12.5fps.nemo.\n",
            "âœ… Model loaded successfully!\n"
          ]
        }
      ],
      "source": [
        "# Load NanoCodec model\n",
        "MODEL_ID = \"nvidia/nemo-nano-codec-22khz-0.6kbps-12.5fps\"\n",
        "SAMPLE_RATE = 22050\n",
        "\n",
        "print(f\"ğŸ“¥ Loading NanoCodec model: {MODEL_ID}\")\n",
        "codec_model = AudioCodecModel.from_pretrained(MODEL_ID).eval()\n",
        "codec_model = codec_model.to(device)\n",
        "print(\"âœ… Model loaded successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## BÆ°á»›c 4: Load Dataset VieNeu-TTS-140h\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ“¦ Loading dataset: pnnbao-ump/VieNeu-TTS-140h [train]\n",
            "   This may take a few minutes...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "53e273972cbf45b5925ffa6da839fec3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0fa413a95ede4828b643bb2207eca69f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "data/train-00000-of-00049.parquet:   0%|          | 0.00/472M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8956c6fc7d91429d88ff551dc8e04a1e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "data/train-00001-of-00049.parquet:   0%|          | 0.00/478M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "35664532dd0c4177adcbf84e47d07e7c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "data/train-00002-of-00049.parquet:   0%|          | 0.00/472M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "acc62384d4d34bf9a5941cdb83ed1254",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "data/train-00003-of-00049.parquet:   0%|          | 0.00/467M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dfb2c443a10f4177b6f76592f7a34e29",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "data/train-00004-of-00049.parquet:   0%|          | 0.00/477M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "55176453324143c9ad03825afce4b544",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "data/train-00005-of-00049.parquet:   0%|          | 0.00/478M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cc9b8f4509cc4717b6b14025db930dc1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "data/train-00006-of-00049.parquet:   0%|          | 0.00/478M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2e6e91c5c5074d3dba4a0e05838d7981",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "data/train-00007-of-00049.parquet:   0%|          | 0.00/479M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ecbbdb01d7af4264aa65f94d5a97e054",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "data/train-00008-of-00049.parquet:   0%|          | 0.00/470M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b50685f6c49447cdb946a0243a904f95",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "data/train-00009-of-00049.parquet:   0%|          | 0.00/472M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "31f80fc46ffb4071a8eb51a9fdfda919",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "data/train-00010-of-00049.parquet:   0%|          | 0.00/474M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "926c1a0b62fd4bc693aa84b8f9e60c47",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "data/train-00011-of-00049.parquet:   0%|          | 0.00/479M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "02b76275fb7d474b9a1767025dbc9fe2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "data/train-00012-of-00049.parquet:   0%|          | 0.00/474M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e6699c03c08747aaa0095b94041ca177",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "data/train-00013-of-00049.parquet:   0%|          | 0.00/475M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d8a08c9fdece477e9e1c0527e47fbfad",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "data/train-00014-of-00049.parquet:   0%|          | 0.00/469M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8176a438971c4472b863a0dda03e36a0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "data/train-00015-of-00049.parquet:   0%|          | 0.00/464M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a643679ea5a54c83859c0154f33c37fa",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "data/train-00016-of-00049.parquet:   0%|          | 0.00/472M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7bc99e9d62354375adaa43cbf9b451e2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "data/train-00017-of-00049.parquet:   0%|          | 0.00/465M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b2041e37024f4996a3ae7e27f796d0f2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "data/train-00018-of-00049.parquet:   0%|          | 0.00/479M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "97f0e6d473ff496ea6a0a03e27528ee5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "data/train-00019-of-00049.parquet:   0%|          | 0.00/472M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ec70b19220fa4a7da25a3ca3ab6ea9c2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "data/train-00020-of-00049.parquet:   0%|          | 0.00/471M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1e4d3c1ca3aa4d788ad4bd42e2bf09b6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "data/train-00021-of-00049.parquet:   0%|          | 0.00/471M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "88c0caba95b5498891b8604244f517cc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "data/train-00022-of-00049.parquet:   0%|          | 0.00/473M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ca0c3145ee6a40b993956d4a2f61b827",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "data/train-00023-of-00049.parquet:   0%|          | 0.00/477M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9a93e5061c144bfd9795dca4fd42c34c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "data/train-00024-of-00049.parquet:   0%|          | 0.00/468M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f75d485eb3e944948614a2bbeaf41b1a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "data/train-00025-of-00049.parquet:   0%|          | 0.00/472M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1ecd5cab78594423832cbc682f8a7ee4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "data/train-00026-of-00049.parquet:   0%|          | 0.00/478M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c08cb5c5958c474e836c1b0cc0908c52",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "data/train-00027-of-00049.parquet:   0%|          | 0.00/465M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e6a0f81069e74506b9c327518caa750b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "data/train-00028-of-00049.parquet:   0%|          | 0.00/470M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e4a994198eb2450db97bd0720761b4e4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "data/train-00029-of-00049.parquet:   0%|          | 0.00/466M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0c78019079964c3b845e18f4776d6a44",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "data/train-00030-of-00049.parquet:   0%|          | 0.00/471M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "04af4651c20d457790879f0326c4c072",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "data/train-00031-of-00049.parquet:   0%|          | 0.00/471M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dc643126e6944171a09287fc627dba8a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "data/train-00032-of-00049.parquet:   0%|          | 0.00/469M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ac314edc6152455fbbeaebf4b751e7a1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "data/train-00033-of-00049.parquet:   0%|          | 0.00/477M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5d83939dbacc46658f8f3aca2798783a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "data/train-00034-of-00049.parquet:   0%|          | 0.00/481M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8f40d07c7f44439d986ab29d4b1e6a53",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "data/train-00035-of-00049.parquet:   0%|          | 0.00/470M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0232b738c70b4f7c94316fb642c79290",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "data/train-00036-of-00049.parquet:   0%|          | 0.00/469M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a3cda5dcd6fd4c888bea1e7e9a6f56b0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "data/train-00037-of-00049.parquet:   0%|          | 0.00/471M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c1082e37c3c34d1db6db44198de2f37b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "data/train-00038-of-00049.parquet:   0%|          | 0.00/474M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ecc499fca3924295b5bca73119891170",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "data/train-00039-of-00049.parquet:   0%|          | 0.00/470M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d73ef788edcd415187d07ebedecb4127",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "data/train-00040-of-00049.parquet:   0%|          | 0.00/475M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5c41a365e24f4880a821b233751ae1ed",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "data/train-00041-of-00049.parquet:   0%|          | 0.00/468M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7527895b3ed94b5e8c2411b2bbe4499d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "data/train-00042-of-00049.parquet:   0%|          | 0.00/468M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "825f12d34c5e4ce5b88d225f12e4f662",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "data/train-00043-of-00049.parquet:   0%|          | 0.00/470M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c0beb21d23a6412fb247db718d9ae596",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "data/train-00044-of-00049.parquet:   0%|          | 0.00/467M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "221bdc78d3f74e0cb0fb3c400c0b86f1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "data/train-00045-of-00049.parquet:   0%|          | 0.00/479M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "837563e32ab648bc938b9decfceaa995",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "data/train-00046-of-00049.parquet:   0%|          | 0.00/477M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f09bf3b247144157b071b8634942877c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "data/train-00047-of-00049.parquet:   0%|          | 0.00/475M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e46aac82484649acb8ee09b3ff73be76",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "data/train-00048-of-00049.parquet:   0%|          | 0.00/470M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Loaded 74858 samples\n",
            "   Sample keys: dict_keys(['_id', 'audio', 'text', 'phonemized_text', 'duration', 'speaker', 'gender', 'language'])\n"
          ]
        }
      ],
      "source": [
        "# Load dataset\n",
        "DATASET_NAME = \"pnnbao-ump/VieNeu-TTS-140h\"\n",
        "SPLIT = \"train\"\n",
        "\n",
        "print(f\"ğŸ“¦ Loading dataset: {DATASET_NAME} [{SPLIT}]\")\n",
        "print(\"   This may take a few minutes...\")\n",
        "\n",
        "dataset = load_dataset(\n",
        "    DATASET_NAME,\n",
        "    split=SPLIT,\n",
        "    verification_mode='no_checks',\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "# Convert audio to target sample rate\n",
        "dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=SAMPLE_RATE))\n",
        "\n",
        "print(f\"âœ… Loaded {len(dataset)} samples\")\n",
        "print(f\"   Sample keys: {dataset[0].keys()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## BÆ°á»›c 5: Xem thá»‘ng kÃª dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ“Š Dataset Statistics:\n",
            "   Total samples: 74,858\n",
            "   Total speakers: 193\n",
            "   Average duration: 6.76 seconds\n",
            "\n",
            "   Gender distribution:\n",
            "     female: 24,658 (32.9%)\n",
            "     male: 50,200 (67.1%)\n",
            "\n",
            "   Top 10 speakers:\n",
            "     jellyfish1010_0004: 974 samples\n",
            "     jellyfish1010_0037: 766 samples\n",
            "     jellyfish1010_0028: 742 samples\n",
            "     alloy1512_1002: 727 samples\n",
            "     capybara1812_0076: 726 samples\n",
            "     jellyfish1010_0010: 715 samples\n",
            "     capybara1812_0025: 694 samples\n",
            "     capybara1812_0027: 685 samples\n",
            "     alloy1512_1005: 668 samples\n",
            "     jellyfish1010_1017: 667 samples\n"
          ]
        }
      ],
      "source": [
        "# Thá»‘ng kÃª dataset\n",
        "from collections import Counter\n",
        "\n",
        "speakers = dataset['speaker']\n",
        "unique_speakers = set(speakers)\n",
        "speaker_counts = Counter(speakers)\n",
        "\n",
        "print(f\"ğŸ“Š Dataset Statistics:\")\n",
        "print(f\"   Total samples: {len(dataset):,}\")\n",
        "print(f\"   Total speakers: {len(unique_speakers)}\")\n",
        "print(f\"   Average duration: {sum(dataset['duration']) / len(dataset):.2f} seconds\")\n",
        "\n",
        "if 'gender' in dataset.column_names:\n",
        "    genders = dataset['gender']\n",
        "    gender_counts = Counter(genders)\n",
        "    print(f\"\\n   Gender distribution:\")\n",
        "    for gender, count in gender_counts.items():\n",
        "        print(f\"     {gender}: {count:,} ({count/len(dataset)*100:.1f}%)\")\n",
        "\n",
        "print(f\"\\n   Top 10 speakers:\")\n",
        "for speaker, count in speaker_counts.most_common(10):\n",
        "    print(f\"     {speaker}: {count:,} samples\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## BÆ°á»›c 6: Encode Audio thÃ nh NanoCodec Tokens\n",
        "\n",
        "HÃ m nÃ y sáº½ encode audio thÃ nh 4 layers cá»§a codec tokens.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ§ª Testing encoding vá»›i 1 máº«u...\n",
            "âœ… Test thÃ nh cÃ´ng!\n",
            "   Encoded length: 88\n",
            "   Layer 1 shape: (88,)\n"
          ]
        }
      ],
      "source": [
        "def encode_audio(waveform, codec_model, device):\n",
        "    \"\"\"Encode audio waveform to NanoCodec tokens\"\"\"\n",
        "    # Convert to tensor\n",
        "    audio_tensor = torch.from_numpy(waveform).unsqueeze(dim=0).to(dtype=torch.float32)\n",
        "    audio_tensor = audio_tensor.to(device)\n",
        "    audio_len = torch.tensor([audio_tensor[0].shape[0]]).to(device)\n",
        "    \n",
        "    # Encode\n",
        "    with torch.inference_mode():\n",
        "        encoded_tokens, encoded_len = codec_model.encode(\n",
        "            audio=audio_tensor, \n",
        "            audio_len=audio_len\n",
        "        )\n",
        "    \n",
        "    # Extract layers\n",
        "    encoded_tokens = encoded_tokens.squeeze()\n",
        "    \n",
        "    return {\n",
        "        'nano_layer_1': encoded_tokens[0].cpu().numpy().flatten(),\n",
        "        'nano_layer_2': encoded_tokens[1].cpu().numpy().flatten(),\n",
        "        'nano_layer_3': encoded_tokens[2].cpu().numpy().flatten(),\n",
        "        'nano_layer_4': encoded_tokens[3].cpu().numpy().flatten(),\n",
        "        'encoded_len': encoded_len.item()\n",
        "    }\n",
        "\n",
        "# Test vá»›i 1 máº«u\n",
        "print(\"ğŸ§ª Testing encoding vá»›i 1 máº«u...\")\n",
        "sample = dataset[0]\n",
        "encoded = encode_audio(sample['audio']['array'], codec_model, device)\n",
        "print(f\"âœ… Test thÃ nh cÃ´ng!\")\n",
        "print(f\"   Encoded length: {encoded['encoded_len']}\")\n",
        "print(f\"   Layer 1 shape: {encoded['nano_layer_1'].shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## BÆ°á»›c 7: Process toÃ n bá»™ dataset\n",
        "\n",
        "âš ï¸ **LÆ°u Ã½:** QuÃ¡ trÃ¬nh nÃ y cÃ³ thá»ƒ máº¥t vÃ i giá» tÃ¹y vÃ o GPU vÃ  sá»‘ lÆ°á»£ng máº«u.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸš€ Báº¯t Ä‘áº§u xá»­ lÃ½ dataset...\n",
            "ğŸ“ Writing to: shards/vieneu-tts-00000.jsonl.gz\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b33a4b2ff86345d68e61f887f528c0ed",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Processing:   0%|          | 0/74858 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\\nğŸ“ Rotated to: shards/vieneu-tts-00001.jsonl.gz\n",
            "\\nğŸ“ Rotated to: shards/vieneu-tts-00002.jsonl.gz\n",
            "\\nğŸ“ Rotated to: shards/vieneu-tts-00003.jsonl.gz\n",
            "\\nğŸ“ Rotated to: shards/vieneu-tts-00004.jsonl.gz\n",
            "\\nğŸ“ Rotated to: shards/vieneu-tts-00005.jsonl.gz\n",
            "\\nğŸ“ Rotated to: shards/vieneu-tts-00006.jsonl.gz\n",
            "\\nğŸ“ Rotated to: shards/vieneu-tts-00007.jsonl.gz\n",
            "\\nâœ… Processing completed!\n",
            "   Generated 8 files\n",
            "âœ… HoÃ n thÃ nh! ÄÃ£ táº¡o 8 files trong shards/\n"
          ]
        }
      ],
      "source": [
        "# Cáº¥u hÃ¬nh xá»­ lÃ½\n",
        "LINES_PER_FILE = 10000  # Sá»‘ máº«u má»—i file\n",
        "BATCH_SIZE = 32  # Process theo batch Ä‘á»ƒ tÄƒng tá»‘c\n",
        "\n",
        "def process_dataset(dataset, codec_model, device, out_dir, lines_per_file=10000):\n",
        "    \"\"\"Process dataset vÃ  lÆ°u thÃ nh cÃ¡c file JSONL.gz\"\"\"\n",
        "    \n",
        "    file_idx = 0\n",
        "    lines_in_file = 0\n",
        "    current_file = None\n",
        "    current_gz = None\n",
        "    \n",
        "    def open_new_file(idx):\n",
        "        path = os.path.join(out_dir, f\"vieneu-tts-{idx:05d}.jsonl.gz\")\n",
        "        gz_file = gzip.open(path, 'wt', encoding='utf-8')\n",
        "        return path, gz_file\n",
        "    \n",
        "    def close_file(gz_file):\n",
        "        if gz_file:\n",
        "            gz_file.close()\n",
        "    \n",
        "    def write_record(record, gz_file):\n",
        "        json.dump(record, gz_file, ensure_ascii=False)\n",
        "        gz_file.write('\\n')\n",
        "    \n",
        "    # Má»Ÿ file Ä‘áº§u tiÃªn\n",
        "    file_path, current_gz = open_new_file(file_idx)\n",
        "    print(f\"ğŸ“ Writing to: {file_path}\")\n",
        "    \n",
        "    total_samples = len(dataset)\n",
        "    pbar = tqdm(total=total_samples, desc=\"Processing\")\n",
        "    \n",
        "    for idx, item in enumerate(dataset):\n",
        "        try:\n",
        "            # Encode audio\n",
        "            encoded = encode_audio(item['audio']['array'], codec_model, device)\n",
        "            \n",
        "            # Táº¡o record\n",
        "            record = {\n",
        "                'text': item['text'],\n",
        "                'nano_layer_1': encoded['nano_layer_1'].tolist(),\n",
        "                'nano_layer_2': encoded['nano_layer_2'].tolist(),\n",
        "                'nano_layer_3': encoded['nano_layer_3'].tolist(),\n",
        "                'nano_layer_4': encoded['nano_layer_4'].tolist(),\n",
        "                'encoded_len': encoded['encoded_len'],\n",
        "                'speaker': item['speaker'],\n",
        "                'lang': 'vi',\n",
        "                'dataset_source': 'vieneu-tts-140h'\n",
        "            }\n",
        "            \n",
        "            # ThÃªm phonemized_text náº¿u cÃ³\n",
        "            if 'phonemized_text' in item:\n",
        "                record['phonemized_text'] = item['phonemized_text']\n",
        "            \n",
        "            # ThÃªm gender náº¿u cÃ³\n",
        "            if 'gender' in item:\n",
        "                record['gender'] = item['gender']\n",
        "            \n",
        "            # Ghi vÃ o file\n",
        "            write_record(record, current_gz)\n",
        "            lines_in_file += 1\n",
        "            pbar.update(1)\n",
        "            \n",
        "            # Rotate file náº¿u cáº§n\n",
        "            if lines_in_file >= lines_per_file:\n",
        "                close_file(current_gz)\n",
        "                file_idx += 1\n",
        "                file_path, current_gz = open_new_file(file_idx)\n",
        "                print(f\"\\\\nğŸ“ Rotated to: {file_path}\")\n",
        "                lines_in_file = 0\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f\"\\\\nâš ï¸  Error processing sample {idx}: {e}\")\n",
        "            continue\n",
        "    \n",
        "    # ÄÃ³ng file cuá»‘i cÃ¹ng\n",
        "    close_file(current_gz)\n",
        "    pbar.close()\n",
        "    \n",
        "    print(f\"\\\\nâœ… Processing completed!\")\n",
        "    print(f\"   Generated {file_idx + 1} files\")\n",
        "    return file_idx + 1\n",
        "\n",
        "# Cháº¡y processing\n",
        "print(\"ğŸš€ Báº¯t Ä‘áº§u xá»­ lÃ½ dataset...\")\n",
        "num_files = process_dataset(dataset, codec_model, device, OUT_DIR, LINES_PER_FILE)\n",
        "print(f\"âœ… HoÃ n thÃ nh! ÄÃ£ táº¡o {num_files} files trong {OUT_DIR}/\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## BÆ°á»›c 8: Assemble dataset tá»« cÃ¡c shards\n",
        "\n",
        "Tá»•ng há»£p táº¥t cáº£ cÃ¡c file JSONL.gz thÃ nh má»™t dataset hoÃ n chá»‰nh.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ”¨ Assembling dataset from shards...\n",
            "   Found 8 shard files\n",
            "âœ… Dataset assembled: 74858 samples\n",
            "   Features: {'text': Value(dtype='string', id=None), 'nano_layer_1': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None), 'nano_layer_2': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None), 'nano_layer_3': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None), 'nano_layer_4': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None), 'encoded_len': Value(dtype='int64', id=None), 'speaker': Value(dtype='string', id=None), 'lang': Value(dtype='string', id=None), 'dataset_source': Value(dtype='string', id=None), 'phonemized_text': Value(dtype='string', id=None), 'gender': Value(dtype='string', id=None)}\n",
            "\n",
            "ğŸ“‹ Sample record:\n",
            "   text: NgÃ i Morrel, báº¡n bÃ¨ vÃ  gia Ä‘Ã¬nh hai há» Ä‘á»u cÃ³ máº·t Ä‘Ã´ng Ä‘á»§ Ä‘á»ƒ chÃºc phÃºc cho Ä‘Ã´i tráº».\n",
            "   nano_layer_1: list with 88 items\n",
            "   nano_layer_2: list with 88 items\n",
            "   nano_layer_3: list with 88 items\n",
            "   nano_layer_4: list with 88 items\n",
            "   encoded_len: 88\n",
            "   speaker: capybara1812_1053\n",
            "   lang: vi\n",
            "   dataset_source: vieneu-tts-140h\n",
            "   phonemized_text: Å‹ËˆaË2j mËˆÉ’É¹É™l, bËˆaË6n bËˆÉ›2 vËŒaË2 zËˆaË É—Ëˆi2É² hËˆaËj hËˆÉ”6 É—Ëˆe2w kËˆÉ”Éœ mËˆa6tÌª É—ËˆoÅ‹ É—Ëˆu4 É—ËŒe4 tÊƒËˆuÉœc fËˆuÉœc tÊƒËŒÉ” É—Ëˆoj tÊƒËˆÉ›4. \n",
            "   gender: female\n"
          ]
        }
      ],
      "source": [
        "# Assemble dataset tá»« cÃ¡c shards\n",
        "from datasets import load_dataset\n",
        "import glob\n",
        "\n",
        "print(\"ğŸ”¨ Assembling dataset from shards...\")\n",
        "\n",
        "# Load táº¥t cáº£ cÃ¡c file JSONL.gz\n",
        "shard_files = sorted(glob.glob(os.path.join(OUT_DIR, \"*.jsonl.gz\")))\n",
        "print(f\"   Found {len(shard_files)} shard files\")\n",
        "\n",
        "# Load dataset tá»« cÃ¡c shards\n",
        "final_dataset = load_dataset(\n",
        "    \"json\",\n",
        "    data_files=shard_files,\n",
        "    split=\"train\",\n",
        "    verification_mode='no_checks'\n",
        ")\n",
        "\n",
        "print(f\"âœ… Dataset assembled: {len(final_dataset)} samples\")\n",
        "print(f\"   Features: {final_dataset.features}\")\n",
        "\n",
        "# Xem má»™t máº«u\n",
        "print(f\"\\nğŸ“‹ Sample record:\")\n",
        "sample = final_dataset[0]\n",
        "for key in sample.keys():\n",
        "    if isinstance(sample[key], list) and len(sample[key]) > 0:\n",
        "        print(f\"   {key}: list with {len(sample[key])} items\")\n",
        "    else:\n",
        "        print(f\"   {key}: {sample[key]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## BÆ°á»›c 9: Upload lÃªn HuggingFace Hub\n",
        "\n",
        "âš ï¸ **LÆ°u Ã½:** \n",
        "- Äáº£m báº£o Ä‘Ã£ login HuggingFace á»Ÿ BÆ°á»›c 1\n",
        "- Thay Ä‘á»•i `HF_USERNAME` vÃ  `DATASET_NAME` thÃ nh tÃªn cá»§a báº¡n\n",
        "- Dataset sáº½ Ä‘Æ°á»£c upload dÆ°á»›i dáº¡ng private máº·c Ä‘á»‹nh (cÃ³ thá»ƒ Ä‘á»•i thÃ nh public)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ“¤ Cáº¥u hÃ¬nh upload:\n",
            "   Repository: pnnbao-ump/VieNeu-TTS-140h-nanocodec\n",
            "   Dataset size: 74,858 samples\n",
            "âœ… Logged in as: pnnbao-ump\n"
          ]
        }
      ],
      "source": [
        "# Cáº¥u hÃ¬nh upload\n",
        "HF_USERNAME = \"pnnbao-ump\"  # âš ï¸ THAY Äá»”I: TÃªn HuggingFace cá»§a báº¡n\n",
        "DATASET_NAME = \"VieNeu-TTS-140h-nanocodec\"  # âš ï¸ THAY Äá»”I: TÃªn dataset báº¡n muá»‘n\n",
        "REPO_ID = f\"{HF_USERNAME}/{DATASET_NAME}\"\n",
        "\n",
        "print(f\"ğŸ“¤ Cáº¥u hÃ¬nh upload:\")\n",
        "print(f\"   Repository: {REPO_ID}\")\n",
        "print(f\"   Dataset size: {len(final_dataset):,} samples\")\n",
        "\n",
        "# Kiá»ƒm tra xem Ä‘Ã£ login chÆ°a\n",
        "from huggingface_hub import whoami\n",
        "try:\n",
        "    user_info = whoami()\n",
        "    print(f\"âœ… Logged in as: {user_info['name']}\")\n",
        "    if user_info['name'] != HF_USERNAME:\n",
        "        print(f\"âš ï¸  Warning: Username mismatch!\")\n",
        "        print(f\"   Config: {HF_USERNAME}, Logged in: {user_info['name']}\")\n",
        "        print(f\"   Sáº½ upload vá»›i username: {user_info['name']}\")\n",
        "        HF_USERNAME = user_info['name']\n",
        "        REPO_ID = f\"{HF_USERNAME}/{DATASET_NAME}\"\n",
        "except Exception as e:\n",
        "    print(f\"âŒ ChÆ°a login HuggingFace!\")\n",
        "    print(f\"   Vui lÃ²ng cháº¡y láº¡i cell BÆ°á»›c 1 Ä‘á»ƒ login\")\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… ÄÃ£ táº¡o dataset card (README.md)\n"
          ]
        }
      ],
      "source": [
        "# Táº¡o dataset card (README.md)\n",
        "dataset_card = f\"\"\"---\n",
        "license: cc-by-nc-4.0\n",
        "task_categories:\n",
        "- text-to-speech\n",
        "language:\n",
        "- vi\n",
        "tags:\n",
        "- tts\n",
        "- text-to-speech\n",
        "- vietnamese\n",
        "- nanocodec\n",
        "- kani-tts\n",
        "- multi-speaker\n",
        "size_categories:\n",
        "- 10K<n<100K\n",
        "---\n",
        "\n",
        "# {DATASET_NAME}\n",
        "\n",
        "Dataset tiáº¿ng Viá»‡t Ä‘Ã£ Ä‘Æ°á»£c encode báº±ng NVIDIA NeMo NanoCodec cho training Kani TTS.\n",
        "\n",
        "## Dataset Details\n",
        "\n",
        "- **Source:** VieNeu-TTS-140h (pnnbao-ump/VieNeu-TTS-140h)\n",
        "- **Total samples:** {len(final_dataset):,}\n",
        "- **Speakers:** 193\n",
        "- **Language:** Vietnamese (vi)\n",
        "- **Codec:** NVIDIA NeMo NanoCodec (22kHz, 0.6kbps, 12.5fps)\n",
        "- **Format:** JSONL.gz vá»›i NanoCodec tokens (4 layers)\n",
        "\n",
        "## Dataset Structure\n",
        "\n",
        "Má»—i máº«u bao gá»“m:\n",
        "- `text`: Text transcription tiáº¿ng Viá»‡t\n",
        "- `nano_layer_1`: Codec tokens layer 1\n",
        "- `nano_layer_2`: Codec tokens layer 2\n",
        "- `nano_layer_3`: Codec tokens layer 3\n",
        "- `nano_layer_4`: Codec tokens layer 4\n",
        "- `encoded_len`: Äá»™ dÃ i sequence tokens\n",
        "- `speaker`: Speaker ID\n",
        "- `lang`: \"vi\"\n",
        "- `dataset_source`: \"vieneu-tts-140h\"\n",
        "- `phonemized_text`: IPA phonemization (náº¿u cÃ³)\n",
        "- `gender`: Giá»›i tÃ­nh speaker (náº¿u cÃ³)\n",
        "\n",
        "## Usage\n",
        "\n",
        "```python\n",
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"{REPO_ID}\")\n",
        "\n",
        "# Xem má»™t máº«u\n",
        "sample = dataset['train'][0]\n",
        "print(sample['text'])\n",
        "print(f\"Speaker: {{sample['speaker']}}\")\n",
        "print(f\"Tokens: {{sample['encoded_len']}}\")\n",
        "```\n",
        "\n",
        "## License\n",
        "\n",
        "CC BY-NC 4.0 - Non-commercial use only\n",
        "\n",
        "## Citation\n",
        "\n",
        "```bibtex\n",
        "@dataset{{vieneu_tts_140h_nanocodec,\n",
        "  title = {{VieNeu-TTS-140h NanoCodec Dataset}},\n",
        "  author = {{Your Name}},\n",
        "  year = {{2025}},\n",
        "  url = {{https://huggingface.co/datasets/{REPO_ID}}}\n",
        "}}\n",
        "```\n",
        "\"\"\"\n",
        "\n",
        "# LÆ°u dataset card\n",
        "with open(\"README.md\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(dataset_card)\n",
        "\n",
        "print(\"âœ… ÄÃ£ táº¡o dataset card (README.md)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "â˜ï¸  Uploading dataset to HuggingFace Hub...\n",
            "   Repository: pnnbao-ump/VieNeu-TTS-140h-nanocodec\n",
            "   This may take a while depending on dataset size...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9519752c9282474191c0ff4f0310a7ee",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Processing Files (0 / 0)                : |          |  0.00B /  0.00B            "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b858cfde93f04ab5a6628e79603ab484",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "New Data Upload                         : |          |  0.00B /  0.00B            "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "03adbafc5eee4304befda566384b4fd6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "                                        :  99%|#########9| 57.8MB / 58.3MB            "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "âœ… Upload completed!\n",
            "   Dataset URL: https://huggingface.co/datasets/pnnbao-ump/VieNeu-TTS-140h-nanocodec\n"
          ]
        }
      ],
      "source": [
        "# Upload dataset lÃªn HuggingFace Hub\n",
        "print(f\"â˜ï¸  Uploading dataset to HuggingFace Hub...\")\n",
        "print(f\"   Repository: {REPO_ID}\")\n",
        "print(f\"   This may take a while depending on dataset size...\")\n",
        "\n",
        "# Upload dataset\n",
        "final_dataset.push_to_hub(\n",
        "    repo_id=REPO_ID,\n",
        "    private=True,  # Äáº·t False náº¿u muá»‘n public\n",
        "    commit_message=\"Upload VieNeu-TTS-140h NanoCodec dataset\"\n",
        ")\n",
        "\n",
        "print(f\"\\nâœ… Upload completed!\")\n",
        "print(f\"   Dataset URL: https://huggingface.co/datasets/{REPO_ID}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Dataset card uploaded!\n",
            "\n",
            "ğŸ‰ HoÃ n thÃ nh! Dataset Ä‘Ã£ Ä‘Æ°á»£c upload lÃªn:\n",
            "   https://huggingface.co/datasets/pnnbao-ump/VieNeu-TTS-140h-nanocodec\n"
          ]
        }
      ],
      "source": [
        "# Upload README.md (dataset card)\n",
        "from huggingface_hub import HfApi\n",
        "\n",
        "api = HfApi()\n",
        "api.upload_file(\n",
        "    path_or_fileobj=\"README.md\",\n",
        "    path_in_repo=\"README.md\",\n",
        "    repo_id=REPO_ID,\n",
        "    repo_type=\"dataset\",\n",
        "    commit_message=\"Add dataset card\"\n",
        ")\n",
        "\n",
        "print(\"âœ… Dataset card uploaded!\")\n",
        "print(f\"\\nğŸ‰ HoÃ n thÃ nh! Dataset Ä‘Ã£ Ä‘Æ°á»£c upload lÃªn:\")\n",
        "print(f\"   https://huggingface.co/datasets/{REPO_ID}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
